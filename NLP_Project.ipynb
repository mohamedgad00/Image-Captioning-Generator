{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing and Importing Libraries\n",
        "\n",
        "In this section, we install and import all the required libraries needed for data loading, preprocessing, model building, training, and evaluation.\n",
        "\n",
        "- We install essential packages like TensorFlow, NumPy, Pandas, Matplotlib, and Scikit-learn.\n",
        "- We import additional built-in libraries such as `os`, `re`, and `pickle`.\n",
        "- We set random seeds to ensure reproducibility of the results.\n",
        "- If running on Google Colab, we also mount Google Drive to access the dataset and save models."
      ],
      "metadata": {
        "id": "TVl7tefz8O9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip3 install numpy pandas matplotlib scikit-learn tensorflow transformers torch Pillow requests pyngrok streamlit deep_translator googletrans==4.0.0-rc1"
      ],
      "metadata": {
        "id": "bh2cYvocyJCg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "11f8a9a9-e157-4775-86c1-5c1e2d86305d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.1.1\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.7-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.45.0-py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting deep_translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.4.26)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.37.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (4.13.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.7)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m149.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m165.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m161.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.2.7-py3-none-any.whl (23 kB)\n",
            "Downloading streamlit-1.45.0-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m145.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m129.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: googletrans\n",
            "\u001b[33m  DEPRECATION: Building 'googletrans' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'googletrans'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17396 sha256=4a09649e527a12944efc3099eea6366e4314febca5ead5564291c9bb13897b6e\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/17/6f/66a045ea3d168826074691b4b787b8f324d3f646d755443fda\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, watchdog, pyngrok, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, idna, hstspreload, h2, pydeck, nvidia-cusparse-cu12, nvidia-cudnn-cu12, httpcore, nvidia-cusolver-cu12, httpx, deep_translator, googletrans, streamlit\n",
            "\u001b[2K  Attempting uninstall: hyperframe\n",
            "\u001b[2K    Found existing installation: hyperframe 6.1.0\n",
            "\u001b[2K    Uninstalling hyperframe-6.1.0:\n",
            "\u001b[2K      Successfully uninstalled hyperframe-6.1.0\n",
            "\u001b[2K  Attempting uninstall: hpack\n",
            "\u001b[2K    Found existing installation: hpack 4.1.0\n",
            "\u001b[2K    Uninstalling hpack-4.1.0:\n",
            "\u001b[2K      Successfully uninstalled hpack-4.1.0\n",
            "\u001b[2K  Attempting uninstall: h11\n",
            "\u001b[2K    Found existing installation: h11 0.16.0\n",
            "\u001b[2K    Uninstalling h11-0.16.0:\n",
            "\u001b[2K      Successfully uninstalled h11-0.16.0\n",
            "\u001b[2K  Attempting uninstall: chardet\n",
            "\u001b[2K    Found existing installation: chardet 5.2.0\n",
            "\u001b[2K    Uninstalling chardet-5.2.0:\n",
            "\u001b[2K      Successfully uninstalled chardet-5.2.0\n",
            "\u001b[2K  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "\u001b[2K    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "\u001b[2K      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "\u001b[2K  Attempting uninstall: nvidia-curand-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "\u001b[2K  Attempting uninstall: nvidia-cufft-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "\u001b[2K    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "\u001b[2K  Attempting uninstall: nvidia-cublas-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "\u001b[2K    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "\u001b[2K  Attempting uninstall: idna\n",
            "\u001b[2K    Found existing installation: idna 3.10\n",
            "\u001b[2K    Uninstalling idna-3.10:\n",
            "\u001b[2K      Successfully uninstalled idna-3.10\n",
            "\u001b[2K  Attempting uninstall: h2\n",
            "\u001b[2K    Found existing installation: h2 4.2.0\n",
            "\u001b[2K    Uninstalling h2-4.2.0:\n",
            "\u001b[2K      Successfully uninstalled h2-4.2.0\n",
            "\u001b[2K  Attempting uninstall: nvidia-cusparse-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "\u001b[2K  Attempting uninstall: httpcore\n",
            "\u001b[2K    Found existing installation: httpcore 1.0.9\n",
            "\u001b[2K    Uninstalling httpcore-1.0.9:\n",
            "\u001b[2K      Successfully uninstalled httpcore-1.0.9\n",
            "\u001b[2K  Attempting uninstall: nvidia-cusolver-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[2K  Attempting uninstall: httpx\n",
            "\u001b[2K    Found existing installation: httpx 0.28.1\n",
            "\u001b[2K    Uninstalling httpx-0.28.1:\n",
            "\u001b[2K      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26/26\u001b[0m [streamlit]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "openai 1.76.2 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "google-genai 1.13.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.13.3 which is incompatible.\n",
            "langsmith 0.3.39 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-3.0.4 deep_translator-1.11.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pydeck-0.9.1 pyngrok-7.2.7 rfc3986-1.5.0 streamlit-1.45.0 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, layers\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Bidirectional, concatenate, Layer, MultiHeadAttention, LayerNormalization, GlobalAveragePooling1D\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.applications import DenseNet201\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"DEBUG: تم استيراد المكتبات بنجاح.\")\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"DEBUG: تم تحميل Google Drive بنجاح.\")\n",
        "    drive_base_path = '/content/drive/MyDrive/'\n",
        "    data_path = os.path.join(drive_base_path, 'captions.txt')\n",
        "    image_dir = os.path.join(drive_base_path, 'Flicker8k_Dataset')\n",
        "    features_path = os.path.join(drive_base_path, 'features.pkl')\n",
        "    tokenizer_path = os.path.join(drive_base_path, 'tokenizer.pkl')\n",
        "    model_weights_dir = os.path.join(drive_base_path, 'ImageCaptioningModels_Org')\n",
        "    os.makedirs(model_weights_dir, exist_ok=True)\n",
        "    print(f\"DEBUG: مسارات: \\n{data_path}\\n{image_dir}\\n{features_path}\\n{tokenizer_path}\\n{model_weights_dir}\")\n",
        "except ModuleNotFoundError:\n",
        "    print(\"DEBUG: لم يتم اكتشاف بيئة Google Colab.\")\n",
        "    data_path = 'captions.txt'\n",
        "    image_dir = 'Flicker8k_Dataset'\n",
        "    features_path = 'features.pkl'\n",
        "    tokenizer_path = 'tokenizer.pkl'\n",
        "    model_weights_dir = 'ImageCaptioningModels_Org'\n",
        "    os.makedirs(model_weights_dir, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmTzRu9TyI_D",
        "outputId": "0e21deef-a0d3-4241-b59c-747bc0d0a18f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: تم استيراد المكتبات بنجاح.\n",
            "Mounted at /content/drive\n",
            "DEBUG: تم تحميل Google Drive بنجاح.\n",
            "DEBUG: مسارات: \n",
            "/content/drive/MyDrive/captions.txt\n",
            "/content/drive/MyDrive/Flicker8k_Dataset\n",
            "/content/drive/MyDrive/features.pkl\n",
            "/content/drive/MyDrive/tokenizer.pkl\n",
            "/content/drive/MyDrive/ImageCaptioningModels_Org\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Loading Captions Data and Processing Image Names and Captions\n",
        "\n",
        "In this section, we load the captions data and preprocess the image names and captions.\n",
        "- The captions are loaded from a `.txt` file.\n",
        "- We clean the image names by removing extra parts (e.g., `#`).\n",
        "- We store the cleaned captions and image names in separate columns.\n",
        "- We also perform some initial checks to ensure that the dataset has the correct structure and data.\n",
        "\n",
        "We also extract unique image names and print some statistics for validation."
      ],
      "metadata": {
        "id": "wxScFpj58sDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Loading Captions Data and Processing Image Names and Captions\n",
        "try:\n",
        "    data = pd.read_csv(data_path, sep='\\t', header=None, names=['image_raw', 'caption_raw'])\n",
        "    print(f\"DEBUG: Captions file loaded successfully with {len(data)} rows.\")\n",
        "    if 'image_raw' not in data.columns or 'caption_raw' not in data.columns:\n",
        "        raise ValueError(\"Columns 'image_raw' or 'caption_raw' are missing!\")\n",
        "    data['image'] = data['image_raw'].astype(str).apply(lambda x: x.split('#')[0])  # Clean image name\n",
        "    data['caption'] = data['caption_raw'].astype(str)  # Clean captions\n",
        "    print(f\"DEBUG: Example cleaned image: {data['image'].iloc[0]}\")\n",
        "    print(f\"DEBUG: Number of unique images: {data['image'].nunique()}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading or preparing captions file: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amKkUnCV81__",
        "outputId": "95fbdb93-e66f-4788-9159-779ce293432e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Captions file loaded successfully with 40455 rows.\n",
            "DEBUG: Example cleaned image: 1000268201_693b08cb0e.jpg\n",
            "DEBUG: Number of unique images: 8091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Text Cleaning and Tokenizer Preparation\n",
        "\n",
        "In this section, we clean the captions and prepare the tokenizer:\n",
        "- Each caption is converted to lowercase, unwanted characters are removed, and it is tokenized.\n",
        "- We add start and end tokens to each caption to define the beginning and end of each sequence.\n",
        "- We also save or load the tokenizer to/from a pickle file, ensuring it is ready for future use."
      ],
      "metadata": {
        "id": "yUGbIKHO83gU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Text Cleaning and Tokenizer Preparation\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'[^a-z ]', '', text)  # Remove non-alphabetic characters\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    return 'startseq ' + text + ' endseq'  # Add start and end tokens\n",
        "\n",
        "data['caption_cleaned'] = data['caption'].apply(clean_text)\n",
        "print(\"DEBUG: Example cleaned caption:\", data['caption_cleaned'].iloc[0])\n",
        "\n",
        "if os.path.exists(tokenizer_path):\n",
        "    with open(tokenizer_path, 'rb') as f:\n",
        "        tokenizer = pickle.load(f)  # Load existing tokenizer\n",
        "    print(\"DEBUG: Tokenizer loaded.\")\n",
        "else:\n",
        "    tokenizer = Tokenizer()  # Create a new tokenizer\n",
        "    tokenizer.fit_on_texts(data['caption_cleaned'])\n",
        "    with open(tokenizer_path, 'wb') as f:\n",
        "        pickle.dump(tokenizer, f)  # Save the tokenizer\n",
        "    print(\"DEBUG: Tokenizer initialized and saved.\")\n",
        "\n",
        "vocab_size  = len(tokenizer.word_index) + 1  # Size of vocabulary\n",
        "max_length  = max(len(txt.split()) for txt in data['caption_cleaned'])  # Max length of captions\n",
        "print(f\"DEBUG: vocab_size={vocab_size} | max_length={max_length}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flVbO5Hg856g",
        "outputId": "da023e6f-11a3-48a3-cf91-9b4650c0dde7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Example cleaned caption: startseq a child in a pink dress is climbing up a set of stairs in an entry way endseq\n",
            "DEBUG: Tokenizer loaded.\n",
            "DEBUG: vocab_size=8768 | max_length=37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Extracting Image Features or Loading from Pickle File\n",
        "\n",
        "In this section, we extract features from the images using a pre-trained DenseNet model.\n",
        "- If the features are already extracted and stored in a pickle file, we load them from the file.\n",
        "- If not, we use DenseNet201 model (without the top layer) to extract features from each image and save them for future use.\n",
        "- Each feature vector represents the image in a 1920-dimensional space."
      ],
      "metadata": {
        "id": "tHykhdTu89kx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Extracting Image Features or Loading from Pickle File\n",
        "print(\"DEBUG: Starting image feature extraction or loading...\")\n",
        "\n",
        "if os.path.exists(features_path):\n",
        "    with open(features_path, 'rb') as f:\n",
        "        features = pickle.load(f)  # Load precomputed image features\n",
        "else:\n",
        "    base_model = DenseNet201(include_top=False, weights='imagenet', pooling='avg', input_shape=(224, 224, 3))  # Load DenseNet model\n",
        "    features = {}\n",
        "    for img_name in data['image'].unique():\n",
        "        img_path = os.path.join(image_dir, img_name)\n",
        "        if not os.path.exists(img_path):\n",
        "            continue\n",
        "        img = load_img(img_path, target_size=(224, 224))  # Load and resize image\n",
        "        img_array = img_to_array(img)\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "        img_array = tf.keras.applications.densenet.preprocess_input(img_array)  # Preprocess image for DenseNet\n",
        "        feature_vector = base_model.predict(img_array, verbose=0)\n",
        "        feature_vector = np.array(feature_vector).squeeze()  # Flatten the feature vector\n",
        "        features[img_name] = feature_vector\n",
        "    with open(features_path, 'wb') as f:\n",
        "        pickle.dump(features, f)  # Save extracted features\n",
        "    print(\"DEBUG: Image features saved.\")\n",
        "\n",
        "# Example feature\n",
        "example_feature = next(iter(features.values()))\n",
        "print(\"DEBUG: Example image feature shape:\", example_feature.shape)\n",
        "print(\"DEBUG: Example feature values (partial):\", example_feature[:10])\n",
        "\n",
        "feature_size = example_feature.shape[-1]  # Size of feature vector\n",
        "print(f\"DEBUG: Feature vector size: {feature_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bhg3BH98_0_",
        "outputId": "f05c237d-2762-42dc-a9ed-cb741a257440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Starting image feature extraction or loading...\n",
            "DEBUG: Example image feature shape: (1, 1920)\n",
            "DEBUG: Example feature values (partial): [[7.8687917e-05 7.3524064e-04 1.1395990e-03 ... 5.6523514e-01\n",
            "  2.2903775e-01 6.9639796e-01]]\n",
            "DEBUG: Feature vector size: 1920\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Converting Captions to Sequences\n",
        "\n",
        "In this section, we convert the cleaned captions into integer sequences:\n",
        "- The tokenizer is used to map each word in the caption to an integer.\n",
        "- The sequences are padded to ensure they all have the same length (max_length)."
      ],
      "metadata": {
        "id": "28Pa5Qgk9Cn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Converting Captions to Sequences\n",
        "sequences = tokenizer.texts_to_sequences(data['caption_cleaned'])  # Convert captions to sequences of integers\n",
        "X_seq = pad_sequences(sequences, maxlen=max_length, padding='post')  # Pad sequences to the same length\n",
        "\n",
        "# Mapping images to their corresponding caption sequences\n",
        "image_to_seq_map = {}\n",
        "for img_name, seq in zip(data['image'], X_seq):\n",
        "    if img_name not in image_to_seq_map:\n",
        "        image_to_seq_map[img_name] = []\n",
        "    image_to_seq_map[img_name].append(seq)"
      ],
      "metadata": {
        "id": "L7UGxyLe9Drj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Splitting Images for Training and Validation\n",
        "\n",
        "In this section, we split the dataset into training and validation sets:\n",
        "- We split the unique image names into a training set (80%) and validation set (20%)."
      ],
      "metadata": {
        "id": "41k7r2lG9GGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Splitting Images for Training and Validation\n",
        "unique_images = data['image'].unique()\n",
        "img_train_names, img_val_names = train_test_split(unique_images, test_size=0.2, random_state=42)  # Split images into train/val\n",
        "print(\"DEBUG: train images:\", len(img_train_names), \"| val images:\", len(img_val_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ur46urJE9IB0",
        "outputId": "9db0849e-2f4c-4d82-b1e6-0fe4b1b00e81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: train images: 6472 | val images: 1619\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Data Generator for Model Training\n",
        "\n",
        "In this section, we define a custom data generator using TensorFlow's `Sequence` class. This generator handles batching and shuffling of image-caption pairs for training and validation. It provides an efficient way to feed data into the model during training by:\n",
        "\n",
        "- **Generating Pairs:** The generator creates pairs of image features and their corresponding caption sequences.\n",
        "- **Teacher Forcing:** During training, the model predicts the next word in the sequence based on the previous words. This is known as teacher forcing, where each step has an input (previous words) and an output (next word).\n",
        "- **Batching:** The generator returns batches of image features, caption input sequences, and target word sequences, which are used for training the model.\n",
        "- **Shuffling:** The generator shuffles the data after each epoch if the `shuffle` parameter is set to `True`.\n",
        "\n",
        "This generator is designed to be used with the Keras `fit` function for model training.\n",
        "\n",
        "#### Key Components:\n",
        "- **`__init__`**: Initializes the generator with parameters like image names, caption sequences, and batch size.\n",
        "- **`make_pairs`**: Creates a list of pairs `(image_name, caption_sequence)` by combining images and their corresponding captions.\n",
        "- **`__len__`**: Returns the number of batches per epoch.\n",
        "- **`on_epoch_end`**: Shuffles the data indices after each epoch if `shuffle` is enabled.\n",
        "- **`__getitem__`**: Retrieves a batch of image-caption pairs, where each pair contains image features and caption sequences, with teacher forcing applied.\n",
        "\n",
        "#### Purpose:\n",
        "The primary purpose of this data generator is to streamline the process of training the image captioning model. It ensures that the model receives the data in batches, with captions split into input-output pairs, and handles image feature extraction efficiently.\n",
        "\n",
        "This generator can be used directly in the model’s `fit` method for training, as shown below:\n",
        "\n",
        "```\n",
        "train_gen = DataGenerator(img_train_names, image_to_seq_map, features, batch_size, max_length, vocab_size)```\n",
        "\n",
        "``` val_gen = DataGenerator(img_val_names, image_to_seq_map, features, batch_size, max_length, vocab_size)```"
      ],
      "metadata": {
        "id": "3lfk5cXl9QfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, img_names, image_to_seq_map, features, batch_size, max_length, vocab_size, shuffle=True):\n",
        "        # Initialize the data generator\n",
        "        self.img_names = img_names\n",
        "        self.image_to_seq_map = image_to_seq_map\n",
        "        self.features = features\n",
        "        self.batch_size = batch_size\n",
        "        self.max_length = max_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        # Prepare a list of (img_name, seq) pairs\n",
        "        self.pairs = self.make_pairs()\n",
        "        self.indices = np.arange(len(self.pairs))\n",
        "        self.on_epoch_end()  # Shuffle data at the end of each epoch if required\n",
        "\n",
        "    def make_pairs(self):\n",
        "        # Create pairs of image names and corresponding caption sequences\n",
        "        pairs = []\n",
        "        for img_name in self.img_names:\n",
        "            if img_name in self.image_to_seq_map and img_name in self.features:\n",
        "                for seq in self.image_to_seq_map[img_name]:\n",
        "                    pairs.append((img_name, seq))  # Add (image_name, caption_sequence) to pairs list\n",
        "        return pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        # Returns the number of batches per epoch\n",
        "        return int(np.ceil(len(self.pairs) / float(self.batch_size)))\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # Shuffle the indices after each epoch (optional)\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Generate a batch of data (X_img, X_seq, y) for training\n",
        "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_pairs = [self.pairs[i] for i in batch_indices]\n",
        "\n",
        "        X_img, X_seq, y = [], [], []\n",
        "        for img_name, seq in batch_pairs:\n",
        "            feature_vector = self.features[img_name]  # Get image features\n",
        "            feature_vector = np.array(feature_vector).squeeze()\n",
        "\n",
        "            # Teacher forcing: each step in the sequence has an input/output\n",
        "            for i in range(1, len(seq)):\n",
        "                in_seq = pad_sequences([seq[:i]], maxlen=self.max_length, padding='post')[0]  # Input sequence\n",
        "                out_word_index = seq[i]  # The next word in the sequence\n",
        "                if out_word_index == 0: continue  # Skip padding token\n",
        "                out_word = to_categorical([out_word_index], num_classes=self.vocab_size)[0]  # Convert to one-hot encoding\n",
        "\n",
        "                X_img.append(feature_vector)  # Append the image feature\n",
        "                X_seq.append(in_seq)  # Append the input sequence\n",
        "                y.append(out_word)  # Append the output (target) word\n",
        "\n",
        "        return (np.array(X_img), np.array(X_seq)), np.array(y)\n",
        "\n",
        "# Initialize the data generators for training and validation\n",
        "batch_size = 32  # You can adjust this value depending on memory constraints\n",
        "train_gen = DataGenerator(img_train_names, image_to_seq_map, features, batch_size, max_length, vocab_size)\n",
        "val_gen   = DataGenerator(img_val_names, image_to_seq_map, features, batch_size, max_length, vocab_size)"
      ],
      "metadata": {
        "id": "yHEcSQxq9R11",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Training the Model\n",
        "\n",
        "This function handles the process of training the image captioning model, saving its weights, and restoring the best model when necessary.\n",
        "\n",
        "#### Key Steps:\n",
        "1. **Check if Weights Exist**: If the model weights already exist (i.e., training has been done previously), the function will load the weights from the file and return the model without retraining it.\n",
        "2. **Train the Model**: If no weights are found, the function will proceed with training the model.\n",
        "   - **Callbacks**:\n",
        "     - **ModelCheckpoint**: Saves the best version of the model based on the validation loss (`val_loss`).\n",
        "     - **EarlyStopping**: Stops training early if the validation loss does not improve for a specified number of epochs (patience), and restores the best weights from the training process.\n",
        "\n",
        "#### Parameters:\n",
        "- **model**: The image captioning model to be trained.\n",
        "- **model_name**: The name of the model, which will be used to save the weights.\n",
        "- **train_gen**: The training data generator that provides the image-caption pairs.\n",
        "- **val_gen**: The validation data generator.\n",
        "- **model_weights_dir**: The directory where the model weights will be saved.\n",
        "- **epochs**: The number of epochs for training (default is 20).\n",
        "\n",
        "#### Function Logic:\n",
        "- If the model weights already exist, they are loaded into the model.\n",
        "- If the weights don't exist, the model is trained with the provided data generators (`train_gen` and `val_gen`) for the specified number of epochs. The best model weights are saved based on the lowest validation loss.\n",
        "\n",
        "#### Example Usage:\n",
        "\n",
        "```\n",
        "history = train_model(model, 'image_captioning_model', train_gen, val_gen, model_weights_dir, epochs=20)```"
      ],
      "metadata": {
        "id": "iJ-MqBSN-MNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, model_name, train_gen, val_gen, model_weights_dir, epochs=15):\n",
        "    model_weights_path = os.path.join(model_weights_dir, f\"{model_name}_full_model.keras\")\n",
        "    print(f\"\\nDEBUG: ----- Training/Loading model: {model_name} -----\")\n",
        "\n",
        "    # Check if the model weights already exist\n",
        "    if os.path.exists(model_weights_path):\n",
        "        print(f\"DEBUG: Found weights file! Loading the model weights...\")\n",
        "        model.load_weights(model_weights_path)\n",
        "        return model\n",
        "\n",
        "    # Train the model if weights don't exist\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        epochs=epochs,\n",
        "        validation_data=val_gen,\n",
        "        callbacks=[\n",
        "            # Save the best model based on validation loss\n",
        "            tf.keras.callbacks.ModelCheckpoint(\n",
        "                model_weights_path, save_best_only=True, save_weights_only=False, monitor='val_loss', verbose=1\n",
        "            ),\n",
        "            # Stop training early if the validation loss doesn't improve\n",
        "            tf.keras.callbacks.EarlyStopping(\n",
        "                monitor='val_loss', patience=3, verbose=1, restore_best_weights=True\n",
        "            )\n",
        "        ],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    print(f\"DEBUG: ----- Model training completed: {model_name} -----\")\n",
        "    return history"
      ],
      "metadata": {
        "id": "o-V8ZRQsyfG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Model Architecture: `build_model1`\n",
        "\n",
        "This function defines the architecture of a simple image captioning model that uses both image features and text sequences (captions). The model consists of two inputs: one for image features and one for the caption sequence. The image features are passed through a dense layer, and the caption sequence goes through an embedding layer followed by an LSTM layer. The outputs of both branches are concatenated and passed through a final dense layer to predict the next word in the sequence.\n",
        "\n",
        "#### Key Components:\n",
        "1. **Image Input**:\n",
        "   - The image features are passed as a vector of shape `(feature_size,)` where `feature_size` is the dimension of the image feature vector extracted earlier using a pre-trained model like DenseNet.\n",
        "   - These features are passed through a Dense layer with 256 units and ReLU activation.\n",
        "\n",
        "2. **Caption Input**:\n",
        "   - The caption input is a sequence of word indices, represented by a vector of shape `(max_length,)`.\n",
        "   - An Embedding layer is used to convert word indices into dense vectors, followed by an LSTM layer to capture the sequential dependencies in the captions.\n",
        "\n",
        "3. **Concatenation of Image and Caption**:\n",
        "   - The outputs of the image feature processing branch (`img_feats`) and the LSTM layer (`lstm`) are concatenated together into a combined feature vector.\n",
        "\n",
        "4. **Output Layer**:\n",
        "   - A Dense layer with softmax activation is used to predict the next word in the sequence. The output dimension is equal to the vocabulary size.\n",
        "\n",
        "#### Parameters:\n",
        "- **feature_size**: The dimension of the image feature vector.\n",
        "- **vocab_size**: The total number of unique words in the vocabulary (plus one for padding).\n",
        "- **max_length**: The maximum length of the caption sequences.\n",
        "\n",
        "#### Function Logic:\n",
        "- The model uses both image features and caption sequences as inputs. The image features are passed through a dense layer, and the captions are processed using an embedding followed by an LSTM.\n",
        "- The outputs of both the image and caption branches are concatenated and passed through a softmax output layer to predict the next word."
      ],
      "metadata": {
        "id": "gAN8O2YzA-ji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model1(feature_size, vocab_size, max_length):\n",
        "    print(f\"feature_size: {feature_size}\")\n",
        "\n",
        "    # Image input branch (input shape is the feature size)\n",
        "    image_input = Input(shape=(feature_size,))\n",
        "    img_feats = Dense(256, activation='relu')(image_input)  # Dense layer for image features\n",
        "\n",
        "    # Caption input branch (input shape is the maximum length of the sequence)\n",
        "    caption_input = Input(shape=(max_length,))\n",
        "    emb = Embedding(vocab_size, 256, mask_zero=True)(caption_input)  # Embedding layer for words\n",
        "    lstm = LSTM(256)(emb)  # LSTM to process the sequence of words\n",
        "\n",
        "    # Combine image and caption features\n",
        "    combined = concatenate([img_feats, lstm])\n",
        "\n",
        "    # Output layer (predict the next word)\n",
        "    output = Dense(vocab_size, activation='softmax')(combined)\n",
        "\n",
        "    # Create and compile the model\n",
        "    model = Model([image_input, caption_input], output)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "9cs_TQ6vzEdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Model Architecture: `build_model2`\n",
        "\n",
        "This function defines a second variation of the image captioning model. The architecture is similar to `build_model1`, but it incorporates a **Bidirectional LSTM** layer instead of a regular LSTM. This allows the model to capture both past and future contexts in the caption sequences, which can improve the model's performance in sequence-based tasks.\n",
        "\n",
        "#### Key Components:\n",
        "1. **Image Input**:\n",
        "   - The image features are passed as a vector of shape `(feature_size,)`, which represents the dimension of the image feature vector extracted earlier. This feature vector is passed through a Dense layer with 256 units and ReLU activation.\n",
        "\n",
        "2. **Caption Input**:\n",
        "   - The caption input is a sequence of word indices with a shape of `(max_length,)`, where `max_length` is the maximum length of the caption sequence.\n",
        "   - An Embedding layer is used to convert word indices into dense vectors of size 256, followed by a **Bidirectional LSTM** layer. The Bidirectional LSTM processes the sequence in both directions (forward and backward), which helps the model understand the context from both past and future words.\n",
        "\n",
        "3. **Concatenation of Image and Caption**:\n",
        "   - The outputs of the image feature processing branch (`img_feats`) and the Bidirectional LSTM layer (`lstm`) are concatenated together into a single vector.\n",
        "\n",
        "4. **Output Layer**:\n",
        "   - A Dense layer with softmax activation is used to predict the next word in the sequence. The output dimension is equal to the vocabulary size.\n",
        "\n",
        "#### Parameters:\n",
        "- **feature_size**: The dimension of the image feature vector.\n",
        "- **vocab_size**: The total number of unique words in the vocabulary (plus one for padding).\n",
        "- **max_length**: The maximum length of the caption sequences.\n",
        "\n",
        "#### Function Logic:\n",
        "- The model takes both image features and caption sequences as input. The image features go through a dense layer, and the captions are processed through an embedding layer followed by a Bidirectional LSTM layer.\n",
        "- The outputs from both the image and caption branches are concatenated and passed through a softmax output layer to predict the next word in the sequence.\n",
        "\n",
        "#### Example Usage:\n",
        "\n",
        "```model = build_model2(feature_size=1920, vocab_size=10000, max_length=40)```\n"
      ],
      "metadata": {
        "id": "kuEU6RHVBS8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model2(feature_size, vocab_size, max_length):\n",
        "    # Image input branch (input shape is the feature size)\n",
        "    image_input = Input(shape=(feature_size,))\n",
        "    img_feats = Dense(256, activation='relu')(image_input)  # Dense layer for image features\n",
        "\n",
        "    # Caption input branch (input shape is the maximum length of the sequence)\n",
        "    caption_input = Input(shape=(max_length,))\n",
        "    emb = Embedding(vocab_size, 256, mask_zero=True)(caption_input)  # Embedding layer for words\n",
        "    lstm = Bidirectional(LSTM(128))(emb)  # Bidirectional LSTM to process the sequence of words\n",
        "\n",
        "    # Combine image and caption features\n",
        "    combined = concatenate([img_feats, lstm])\n",
        "\n",
        "    # Output layer (predict the next word)\n",
        "    output = Dense(vocab_size, activation='softmax')(combined)\n",
        "\n",
        "    # Create and compile the model\n",
        "    model = Model([image_input, caption_input], output)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "sK7G7Jc6zR5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. Model Architecture: `build_model3` with Attention Layer\n",
        "\n",
        "In this model, we introduce an **Attention Layer** to enhance the performance of the image captioning model. The attention mechanism helps the model focus on important parts of the image features and caption during training, which can improve its ability to generate more accurate and relevant captions.\n",
        "\n",
        "#### Key Components:\n",
        "1. **Image Input**:\n",
        "   - The image features are passed as a vector of shape `(feature_size,)`, which represents the dimension of the image feature vector. This feature vector is passed through a Dense layer with 256 units and ReLU activation.\n",
        "\n",
        "2. **Caption Input**:\n",
        "   - The caption input is a sequence of word indices with a shape of `(max_length,)`, where `max_length` is the maximum length of the caption sequence.\n",
        "   - An Embedding layer is used to convert word indices into dense vectors of size 256. These embeddings are then passed through an LSTM layer with 256 units. The `return_sequences=True` argument ensures that the LSTM outputs sequences for every time step, not just the final state.\n",
        "\n",
        "3. **Attention Mechanism**:\n",
        "   - The attention layer takes the image features (`img_feats`) and the output from the LSTM layer (`lstm`). The attention mechanism computes attention scores to focus on important parts of the image features for each word in the caption.\n",
        "   - The attention weights are computed using a dense layer (`V`) that scores the combination of image features and LSTM outputs. The context vector is then calculated as a weighted sum of the LSTM outputs, based on the attention weights.\n",
        "\n",
        "4. **Output Layer**:\n",
        "   - The context vector from the attention mechanism is passed through a Dense layer with softmax activation, which predicts the next word in the caption sequence.\n",
        "\n",
        "#### Parameters:\n",
        "- **feature_size**: The dimension of the image feature vector.\n",
        "- **vocab_size**: The total number of unique words in the vocabulary (plus one for padding).\n",
        "- **max_length**: The maximum length of the caption sequences.\n",
        "\n",
        "#### Function Logic:\n",
        "- The model takes both image features and caption sequences as input. The image features go through a dense layer, and the captions are processed through an embedding layer followed by an LSTM layer.\n",
        "- The attention layer computes the attention weights and context vector, which are then passed through a softmax output layer to predict the next word in the sequence."
      ],
      "metadata": {
        "id": "Y7wl9QyzBsTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Layer, Dense\n",
        "import tensorflow as tf\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    def __init__(self, units, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        # Initialize the sub-layers\n",
        "        self.W1 = Dense(units)\n",
        "        self.W2 = Dense(units)\n",
        "        self.V = Dense(1)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super().build(input_shape)  # Automatically build sub-layers\n",
        "\n",
        "    def call(self, inputs):\n",
        "        features, hidden = inputs\n",
        "        features_expanded = tf.expand_dims(features, 1)\n",
        "        hidden_dense = self.W2(hidden)\n",
        "        features_dense = self.W1(features_expanded)\n",
        "        score = tf.nn.tanh(features_dense + hidden_dense)\n",
        "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
        "        context_vector = attention_weights * hidden\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        features_shape, hidden_shape = input_shape\n",
        "        context_vector_shape = (features_shape[0], hidden_shape[2])\n",
        "        attention_weights_shape = (features_shape[0], hidden_shape[1], 1)\n",
        "        return [context_vector_shape, attention_weights_shape]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"units\": self.units})\n",
        "        return config\n",
        "\n",
        "def build_model3(feature_size, vocab_size, max_length):\n",
        "    # Image input branch (input shape is the feature size)\n",
        "    image_input = Input(shape=(feature_size,))\n",
        "    img_feats = Dense(256, activation='relu')(image_input)  # Dense layer for image features\n",
        "\n",
        "    # Caption input branch (input shape is the maximum length of the sequence)\n",
        "    caption_input = Input(shape=(max_length,))\n",
        "    emb = Embedding(vocab_size, 256, mask_zero=True)(caption_input)  # Embedding layer for words\n",
        "    lstm = LSTM(256, return_sequences=True)(emb)  # LSTM layer to process the sequence of words\n",
        "\n",
        "    # Attention mechanism\n",
        "    attention = AttentionLayer(256)\n",
        "    context_vector, _ = attention([img_feats, lstm])\n",
        "\n",
        "    # Output layer (predict the next word)\n",
        "    output = Dense(vocab_size, activation='softmax')(context_vector)\n",
        "\n",
        "    # Create and compile the model\n",
        "    model = Model([image_input, caption_input], output)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "Mt0vRK7IzU2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `build_model4` with Transformer Decoder\n",
        "\n",
        "In this model, we combine a Transformer Decoder to handle both image features and text (captions). This model utilizes the Transformer mechanism, which is commonly used in tasks that involve sequence-to-sequence transformation, such as machine translation and image captioning.\n",
        "\n",
        "## **Model Components:**\n",
        "\n",
        "### **Image Input:**\n",
        "The image features are passed as a vector of size `feature_size`. This vector is projected into a 256-dimensional space using a Dense layer.\n",
        "\n",
        "### **Caption Input:**\n",
        "The caption input consists of word indices of shape `(max_length,)`, where `max_length` is the length of the longest caption.\n",
        "\n",
        "These word indices are then converted into dense vector representations using an Embedding layer.\n",
        "\n",
        "### **RepeatFeatures Layer:**\n",
        "This custom layer is used to repeat the image features to match the length of the caption sequence. This step is necessary to model the relationship between the image features and the words in the caption at each step.\n",
        "\n",
        "### **Transformer Decoder:**\n",
        "The function `transformer_decoder` processes both the caption and image features using the Transformer mechanism.\n",
        "\n",
        "- The image features are projected into a 256-dimensional space, and then they are repeated to match the caption length.\n",
        "- The caption is also projected into a 256-dimensional space.\n",
        "- These projections are combined using the Add layer.\n",
        "- The combined features are passed through the Multi-Head Attention layer to learn the relationships between the image features and the caption words.\n",
        "- After attention, LayerNormalization is applied to stabilize the learning.\n",
        "- Finally, the output is passed through a Feed-Forward Network (FFN) to enhance the representation.\n",
        "\n",
        "### **Global Average Pooling:**\n",
        "After the Transformer decoding process, **GlobalAveragePooling1D** is applied to obtain a fixed-size vector representation from the resulting sequence.\n",
        "\n",
        "### **Output Layer:**\n",
        "The final output is passed through a Dense layer with a softmax activation function to predict the next word in the sequence.\n",
        "\n",
        "## **Model Explanation:**\n",
        "\n",
        "**Parameters:**\n",
        "- `feature_size`: The size of the image feature vector.\n",
        "- `vocab_size`: The total number of words in the vocabulary (including padding).\n",
        "- `max_length`: The maximum allowed length for a caption sequence.\n"
      ],
      "metadata": {
        "id": "S6I9WJXRCWCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Layer, Dense, Embedding, Input, MultiHeadAttention, GlobalAveragePooling1D\n",
        "import tensorflow as tf\n",
        "\n",
        "# Repeat the image features to match the sequence length\n",
        "class RepeatFeatures(Layer):\n",
        "    def __init__(self, max_length, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def call(self, inputs):\n",
        "        features_expanded = tf.expand_dims(inputs, 1)  # Expanding the feature to match sequence length\n",
        "        return tf.repeat(features_expanded, repeats=self.max_length, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.max_length, input_shape[-1])\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"max_length\": self.max_length})\n",
        "        return config\n",
        "\n",
        "# Transformer Decoder function\n",
        "def transformer_decoder(text_embeddings, image_features, max_length):\n",
        "    img_proj = Dense(256)(image_features)  # Project image features to match text embeddings size\n",
        "    img_repeated = RepeatFeatures(max_length)(img_proj)  # Repeat the image features to match caption sequence length\n",
        "    text_proj = Dense(256)(text_embeddings)  # Project text embeddings to match image features size\n",
        "    combined = layers.Add()([text_proj, img_repeated])  # Combine both projections\n",
        "\n",
        "    # Multi-Head Attention\n",
        "    attn_output = MultiHeadAttention(num_heads=2, key_dim=256)(combined, combined)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(attn_output + combined)  # Normalize the output\n",
        "\n",
        "    # Feed-Forward Network (FFN)\n",
        "    ffn = Dense(512, activation='relu')(x)\n",
        "    ffn = Dense(256)(ffn)  # Another dense layer\n",
        "    output = layers.LayerNormalization(epsilon=1e-6)(ffn + x)  # Normalize the final output\n",
        "    return output\n",
        "\n",
        "# Building the model\n",
        "def build_model4(feature_size, vocab_size, max_length):\n",
        "    image_input = Input(shape=(feature_size,))  # Image feature input\n",
        "    img_feats = Dense(256)(image_input)  # Project image features to a 256-dimensional space\n",
        "\n",
        "    caption_input = Input(shape=(max_length,))  # Caption input\n",
        "    emb = Embedding(vocab_size, 256, mask_zero=True)(caption_input)  # Embed the words in the captions\n",
        "\n",
        "    trans_out = transformer_decoder(emb, img_feats, max_length)  # Pass through the transformer decoder\n",
        "    trans_out = GlobalAveragePooling1D()(trans_out)  # Global average pooling to get a fixed-size vector\n",
        "\n",
        "    output = Dense(vocab_size, activation='softmax')(trans_out)  # Output layer to predict the next word\n",
        "    model = Model([image_input, caption_input], output)  # Create the model\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # Compile the model\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "UEO5pt2gzV0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training `model1` and `model2`\n",
        "\n",
        "In this section, we train two models with different architectures: **Model1** using a simple LSTM layer and **Model2** using a Bidirectional LSTM (BiLSTM) layer.\n",
        "\n",
        "#### **Training `model1`:**\n",
        "\n",
        "1. **Building the Model:**\n",
        "   - `build_model1(feature_size, vocab_size, max_length)`: This function builds the model with a simple LSTM layer after embedding the captions.\n",
        "   - `feature_size`: The dimension of the image features passed to the model.\n",
        "   - `vocab_size`: The size of the vocabulary, which is the total number of unique words.\n",
        "   - `max_length`: The maximum sequence length for the captions.\n",
        "\n",
        "2. **Training the Model:**\n",
        "   - `train_model(model1, \"model1_lstm\", train_gen, val_gen, model_weights_dir, epochs=15)`: This trains the model for 15 epochs. It uses the `train_gen` (training data generator) and `val_gen` (validation data generator) to feed the model and monitor performance on the validation set.\n",
        "   - Model weights are saved with the name `\"model1_lstm\"`.\n",
        "\n",
        "#### **Training `model2`:**\n",
        "\n",
        "1. **Building the Model:**\n",
        "   - `build_model2(feature_size, vocab_size, max_length)`: This function builds the model with a Bidirectional LSTM layer.\n",
        "   - Similar to `model1`, this model also uses the `feature_size`, `vocab_size`, and `max_length` parameters.\n",
        "\n",
        "2. **Training the Model:**\n",
        "   - `train_model(model2, \"model2_bilstm\", train_gen, val_gen, model_weights_dir, epochs=15)`: This trains the Bidirectional LSTM model for 15 epochs and saves the weights with the name `\"model2_bilstm\"`.\n",
        "\n",
        "### Key Points:\n",
        "- **LSTM vs BiLSTM**: The key difference between `model1` and `model2` is the architecture. While `model1` uses a regular LSTM layer, `model2` utilizes a Bidirectional LSTM, which processes the input sequence in both forward and backward directions. This allows the model to capture context from both the past and future words in a sentence."
      ],
      "metadata": {
        "id": "ovVTZmbPDLYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model using a simple LSTM layer\n",
        "model1 = build_model1(feature_size, vocab_size, max_length)\n",
        "\n",
        "# Train the model for 15 epochs\n",
        "# `train_model` function will handle the training process and saving the model weights\n",
        "train_model(model1, \"model1_lstm\", train_gen, val_gen, model_weights_dir, epochs=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1cTpwAlzXmV",
        "outputId": "12d684e8-6a8f-42a1-f155-733e00112b67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "feature_size: 1920\n",
            "\n",
            "DEBUG: ----- Training/Loading model: model1_lstm -----\n",
            "DEBUG: Found weights file! Loading the model weights...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Functional name=functional, built=True>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model using a Bidirectional LSTM layer\n",
        "model2 = build_model2(feature_size, vocab_size, max_length)\n",
        "\n",
        "# Train the model for 15 epochs\n",
        "# `train_model` function will handle the training process and saving the model weights\n",
        "train_model(model2, \"model2_bilstm\", train_gen, val_gen, model_weights_dir, epochs=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNJpOP7dDfUn",
        "outputId": "37fd5fcc-d02c-44af-a9e9-0345df5b5ee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DEBUG: ----- Training/Loading model: model2_bilstm -----\n",
            "DEBUG: Found weights file! Loading the model weights...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 24 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Functional name=functional_1, built=True>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training `model3` with Attention Mechanism\n",
        "\n",
        "In this section, we train **Model3** using an Attention mechanism to enhance the caption generation process by focusing on relevant parts of the image and sequence.\n",
        "\n",
        "#### **Training `model3`:**\n"
      ],
      "metadata": {
        "id": "qrDdwbFODrWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model using the Attention mechanism\n",
        "model3 = build_model3(feature_size, vocab_size, max_length)\n",
        "\n",
        "# Train the model for 15 epochs\n",
        "# `train_model` function will handle the training process and saving the model weights\n",
        "train_model(model3, \"model3_attention\", train_gen, val_gen, model_weights_dir, epochs=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "6mZTWRIxGKU0",
        "outputId": "2558ef17-435e-4c98-db52-9d5fe3768540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'attention_layer' (of type AttentionLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DEBUG: ----- Training/Loading model: model3_attention -----\n",
            "Epoch 1/15\n",
            "\u001b[1m 117/1012\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:22\u001b[0m 360ms/step - accuracy: 0.0946 - loss: 7.2686"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-6cfe220352bf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Train the model for 15 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# `train_model` function will handle the training process and saving the model weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model3_attention\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_weights_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-c21471fd00b0>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, model_name, train_gen, val_gen, model_weights_dir, epochs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Train the model if weights don't exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     history = model.fit(\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training `model4` with Transformer Decoder\n",
        "\n",
        "In this section, we train **Model4** which uses a simplified Transformer Decoder to handle both the image features and the caption sequence. The Transformer mechanism is widely used in sequence-to-sequence tasks, providing better performance by learning long-range dependencies in the data.\n",
        "\n",
        "#### **Training `model4`:**"
      ],
      "metadata": {
        "id": "nIGOkIS_D2UT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model using the Transformer Decoder\n",
        "model4 = build_model4(feature_size, vocab_size, max_length)\n",
        "\n",
        "# Train the model for 15 epochs\n",
        "# `train_model` function will handle the training process and saving the model weights\n",
        "train_model(model4, \"model4_transformer_simplified\", train_gen, val_gen, model_weights_dir, epochs=15)"
      ],
      "metadata": {
        "id": "e_nf5NSQ8zUt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "6d6c2b74-eca1-4064-c1db-91eb762637e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DEBUG: ----- Training/Loading model: model4_transformer_simplified -----\n",
            "Epoch 1/15\n",
            "\u001b[1m  50/1012\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14:21\u001b[0m 895ms/step - accuracy: 0.0891 - loss: 7.1057"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-0ea70065cf9e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Train the model for 15 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# `train_model` function will handle the training process and saving the model weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model4_transformer_simplified\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_weights_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-c21471fd00b0>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, model_name, train_gen, val_gen, model_weights_dir, epochs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Train the model if weights don't exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     history = model.fit(\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ngrok Setup and Streamlit App Launch\n",
        "\n",
        "This section sets up Ngrok to expose your local Streamlit app to the internet and then runs the app in the background.\n",
        "\n",
        "#### Step-by-Step Breakdown:\n",
        "\n",
        "1. **`ngrok.kill()`**:\n",
        "   - This command stops any currently running ngrok tunnels to ensure you start with a clean slate.\n",
        "\n",
        "2. **`ngrok.set_auth_token()`**:\n",
        "   - This sets your ngrok authentication token, which is required to authenticate your session with ngrok's servers. Make sure to keep your token secure.\n",
        "\n",
        "3. **`public_url = ngrok.connect(8501)`**:\n",
        "   - This opens a tunnel to the local port 8501 (Streamlit's default port). It then stores the public URL for accessing your app.\n",
        "\n",
        "4. **`print Statements`**:\n",
        "   - These print out a message in the console with the public URL of the app that you can open in a web browser.\n",
        "\n",
        "5. **`app_path = \"/content/app.py\"`**:\n",
        "   - This specifies the location of the Streamlit app file that you want to run.\n",
        "\n",
        "6. **`os.system(f\"streamlit run {app_path} --server.port 8501 --server.headless true &\")`**:\n",
        "   - This launches the Streamlit app in the background on the specified port (8501). The `--server.headless true` flag ensures that the app runs without opening a browser window automatically.\n",
        "\n",
        "---\n",
        "\n",
        "### Things to Note:\n",
        "- **ngrok**: This is used to expose a local server to the internet, which is useful when running on a remote environment like Google Colab.\n",
        "- **Port 8501**: Streamlit by default runs on this port. If you use a different port, make sure to update both the `ngrok.connect()` call and the `streamlit run` command accordingly.\n"
      ],
      "metadata": {
        "id": "OgsM5jTwFKYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Kill any existing ngrok processes\n",
        "ngrok.kill()\n",
        "\n",
        "# Set ngrok authentication token (replace with your own token)\n",
        "ngrok.set_auth_token('2wT3kV38165t2IDIM92nXIBVjbT_5MaG5c41bMsjzPgBUcKaF')\n",
        "\n",
        "# Open a new tunnel on port 8501 (default for Streamlit)\n",
        "delay_seconds = 2\n",
        "print(f\"Waiting for {delay_seconds} seconds before starting a new tunnel...\")\n",
        "time.sleep(delay_seconds) # Stop ex. during 2 secs\n",
        "public_url = ngrok.connect(8501)\n",
        "\n",
        "# Print the public URL\n",
        "print(f\"✅ Streamlit app public URL (open in your browser): {public_url}\")\n",
        "\n",
        "# Define the path to your Streamlit app\n",
        "app_path = \"/content/drive/MyDrive/app.py\"\n",
        "\n",
        "# Run the Streamlit app in the background\n",
        "os.system(f\"streamlit run {app_path} --server.port 8501 --server.headless true &\")\n"
      ],
      "metadata": {
        "id": "tJq6xpqULBXh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c51caed3-978e-40da-b0e4-54c3489ff28e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting for 2 seconds before starting a new tunnel...\n",
            "✅ Streamlit app public URL (open in your browser): NgrokTunnel: \"https://d719-34-16-201-201.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}